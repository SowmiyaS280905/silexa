# SILEXA - Sign Language Detection Web Application

A modern web-based sign language detection system powered by AI and computer vision.

## Features

ğŸ¥ **Real-time Detection**: Instantly recognize sign language gestures using your webcam
ğŸ§  **AI-Powered**: Advanced machine learning algorithms for reliable recognition  
ğŸ“Š **Data Collection**: Easy interface to collect and label new gesture data
ğŸ”„ **Model Training**: Retrain the model with new data directly from the web interface
ğŸŒ **Web-Based**: No installation required - runs in your browser
ğŸ“± **Responsive**: Works on desktop, tablet, and mobile devices

## Supported Gestures

- A, B, C (letters)
- Hi, Hello (greetings)
- No, Stop (negatives)
- Thumbs Up (positive)
- Surprised, Thinking (emotions)

## Quick Start

### Option 1: Simple Demo (Recommended for testing)

```bash
python simple_app.py
```

This will start a demo version with simulated predictions.

### Option 2: Full Application (Requires trained model)

```bash
python app.py
```

This requires a trained model (`model.pkl`) and gesture data (`gestures.csv`).

### Option 3: Train Model First

If you don't have a trained model:

```bash
# 1. Collect some gesture data
python collect_data..py

# 2. Train the model
python train_model.py

# 3. Run the web application
python app.py
```

## Installation

1. **Clone or download** this repository
2. **Install dependencies**:
   ```bash
   pip install -r requirements.txt
   ```
3. **Run the application**:
   ```bash
   python simple_app.py
   ```
4. **Open your browser** to `http://localhost:5000`

## Usage

### 1. Home Page
- Overview of the system
- Navigation to different features
- System statistics

### 2. Real-time Detection (`/detect`)
- Live camera feed with gesture recognition
- Audio feedback for detected gestures
- History of recent detections
- Settings for audio and visual feedback

### 3. Data Collection (`/collect`)
- Capture new gesture samples
- Label gestures for training
- Auto-capture mode for bulk data collection
- Model retraining interface

## Web Interface

### Main Features:
- **Responsive Design**: Works on all screen sizes
- **Modern UI**: Clean, professional interface with gradients and animations
- **Real-time Processing**: 30fps gesture detection
- **Audio Feedback**: Text-to-speech for detected gestures
- **Visual Feedback**: Hand landmark visualization
- **Progress Tracking**: Capture counts and training progress

### Browser Requirements:
- **Camera Access**: Required for gesture detection
- **Modern Browser**: Chrome, Firefox, Safari, Edge (latest versions)
- **JavaScript Enabled**: Required for MediaPipe processing
- **HTTPS**: May be required for camera access on some browsers

## API Endpoints

### `/api/predict` (POST)
Predict gesture from hand landmarks
```json
{
  "landmarks": [x1, y1, x2, y2, ...]
}
```

### `/api/save_gesture` (POST)
Save gesture data for training
```json
{
  "landmarks": [x1, y1, x2, y2, ...],
  "label": "gesture_name"
}
```

### `/api/retrain` (POST)
Retrain the model with new data
```json
{}
```

## File Structure

```
silexa_sign_detection/
â”œâ”€â”€ app.py                 # Main Flask application
â”œâ”€â”€ simple_app.py          # Demo version
â”œâ”€â”€ train_model.py         # Model training script
â”œâ”€â”€ detect_sign.py         # Desktop detection script
â”œâ”€â”€ collect_data..py       # Desktop data collection
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ model.pkl             # Trained ML model
â”œâ”€â”€ gestures.csv          # Training data
â”œâ”€â”€ templates/            # HTML templates
â”‚   â”œâ”€â”€ index.html        # Home page
â”‚   â”œâ”€â”€ detect.html       # Detection interface
â”‚   â””â”€â”€ collect.html      # Data collection interface
â””â”€â”€ static/               # CSS and assets
    â””â”€â”€ style.css         # Custom styles
```

## Technology Stack

- **Backend**: Flask (Python)
- **Frontend**: HTML5, CSS3, JavaScript
- **ML**: scikit-learn, MediaPipe
- **Computer Vision**: OpenCV, MediaPipe
- **UI Framework**: Bootstrap 5
- **Icons**: Font Awesome

## Development

### Adding New Gestures:
1. Go to `/collect` page
2. Enter gesture name
3. Capture multiple samples (10-20 recommended)
4. Click "Retrain Model"
5. New gesture will be available in detection

### Customizing the Interface:
- Edit HTML templates in `templates/`
- Modify styles in `static/style.css`
- Update Flask routes in `app.py`

## Troubleshooting

### Camera Not Working:
- Ensure camera permissions are granted
- Try HTTPS instead of HTTP
- Check if camera is being used by another application

### Model Not Loading:
- Run `python train_model.py` to create model
- Ensure `gestures.csv` exists with training data
- Check file permissions

### Slow Performance:
- Use `simple_app.py` for demo without heavy ML processing
- Reduce MediaPipe model complexity
- Close other applications using camera

## License

This project is for educational and research purposes.

## Contributing

Feel free to submit issues and enhancement requests!

---

**SILEXA** - Making sign language accessible through technology ğŸ¤Ÿ
